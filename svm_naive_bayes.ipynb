{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae204607-cdcd-4607-bb6e-e18e0b2743c6",
   "metadata": {},
   "source": [
    "# Text classification with SVM and naive bayes on Amazon Cellphone Review Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7bc6faa-5115-4463-ab64-032002ba2efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ghost/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ghost/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# text classification with svm and naive bayes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# initialize a random seed\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80af40f9-2fe1-4f06-97d7-e4f94b196386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       product_title  \\\n",
      "0  Mobile Action MA730 Handset Manager - Bluetoot...   \n",
      "1  Mobile Action MA730 Handset Manager - Bluetoot...   \n",
      "2  Mobile Action MA730 Handset Manager - Bluetoot...   \n",
      "3   USB Data Cable for Sony-Ericsson Z600, Z500, ...   \n",
      "4   USB Data Cable for Sony-Ericsson Z600, Z500, ...   \n",
      "\n",
      "                                      review_summary  \\\n",
      "0                                         Don't buy!   \n",
      "1  Mobile Action Bluetooth Mobile Phone Tool Soft...   \n",
      "2                                               good   \n",
      "3                        No instructions included...   \n",
      "4                                   NOT A DATA CABLE   \n",
      "\n",
      "                                         review_text  review_score  \n",
      "0   First of all, the company took my money and s...             1  \n",
      "1  Great product- tried others and this is a ten ...             5  \n",
      "2  works real good....a little hard to set up...w...             4  \n",
      "3   The price was right for this cable ($11.95+$4...             4  \n",
      "4  this is NOT a DATA CABLE this is only a USB ch...             1  \n"
     ]
    }
   ],
   "source": [
    "# import amazon dataset\n",
    "Amazon_Reviews = pd.read_csv(\"amazon.csv\",encoding='latin-1')\n",
    "print(Amazon_Reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c59b1-31a1-4f64-9a1c-eacae874c1bc",
   "metadata": {},
   "source": [
    "# text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa385d9-4d8a-4adf-b6d5-c0c58290728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove blank rows if any.\n",
    "Amazon_Reviews['review_text'].dropna(inplace=True)\n",
    "# make all the text to lower case.  \n",
    "Amazon_Reviews['review_text']  = [entry.lower() for entry in Amazon_Reviews['review_text']]\n",
    "# tokenization each entry in the corpus will be broken into set of words\n",
    "Amazon_Reviews['review_text'] = [word_tokenize(entry) for entry in Amazon_Reviews['review_text'] ]\n",
    "# remove stopwords, non-numeric and perfom word stemming/lemmenting (use WordNetLemmatizer)  \n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(Amazon_Reviews['review_text']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # check for stopwords and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # final processed set of words for each iteration \n",
    "    Amazon_Reviews.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770f2d5-bcaa-49fa-b450-e598ff0bdceb",
   "metadata": {},
   "source": [
    "# Prepare Train and Test Data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba2455-df4a-44f0-8a46-acf02ea312e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels are Amazon product names\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Amazon_Reviews['text_final'],Amazon_Reviews['product_title'],test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aea618-9f48-4e2a-ad32-dde6bc1cfe3c",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ac22c-f8a5-46c0-ae4d-f29ebbc518f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e6c2ac-9e61-4d0e-9876-5e454ad6d77d",
   "metadata": {},
   "source": [
    "# Word Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f207dcc-0b89-4a0d-a738-5a2ad720b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397fc3e-eb4e-4754-b800-983fc5b32bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Tfidf_vect.vocabulary_)\n",
    "# see what vectorized data looks like\n",
    "print(Train_X_Tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6556d91-ce60-4333-854e-580c8bdf8d50",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f47d8-1ff2-4196-bdfd-39c14fe6adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive-Bayes Classifier\n",
    "\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "# get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score: \",accuracy_score(predictions_NB, Test_Y)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907c718-2ae1-41b4-980e-92c248e36961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score: \",accuracy_score(predictions_SVM, Test_Y)*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
